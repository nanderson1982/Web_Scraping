{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping - UFC.com"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URLs to be scraped\n",
    "url_base = \"http://www.ufcstats.com/statistics/fighters?char=\"\n",
    "url_page = \"&page=all\"\n",
    "url = \"http://www.ufcstats.com/statistics/fighters?char=a\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fighter Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_body(url: str):\n",
    "    \"\"\"Send get request to url to get html text and find the table on the webpage.\"\"\"\n",
    "    \n",
    "    # Send get request to URL provided\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # Return the html text of the page\n",
    "    soup = bs(page.text, 'lxml')\n",
    "\n",
    "    # Find the table of data on the page \n",
    "    table_body = soup.find('table')\n",
    "    \n",
    "    return table_body, soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fighter_column_headers(url):\n",
    "    \"\"\"Get headers for the table on webpage.\"\"\"\n",
    "        # Send get request to URL provided\n",
    "    page = requests.get(url)\n",
    "\n",
    "    # Return the html text of the page\n",
    "    soup = bs(page.text, 'lxml')\n",
    "    \n",
    "    column_headers_tag = soup.find('tr', class_= \"b-statistics__table-row\")\n",
    "    column_headers = [header.text.strip() for header in column_headers_tag.find_all(['th', 'td'])]    \n",
    "    return column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fighter_details(table_body: bs):\n",
    "    \"\"\"Get fighter details from a single web page.\"\"\"\n",
    "    \n",
    "    # Empty lists to store data\n",
    "    table_data = []\n",
    "    current_row = []\n",
    "\n",
    "    # Find HTML where table data is listed\n",
    "    for row in table_body.find_all('tr'):\n",
    "        cols = row.find_all('td')\n",
    "        # Enumerate through each row/column\n",
    "        for i, ele in enumerate(cols, start=1):\n",
    "            col = ele.text.strip()\n",
    "            current_row.append(col)\n",
    "\n",
    "            # When i reaches 10, add current row to row_data and reset the current_row list\n",
    "            if i == 10:\n",
    "                table_data.append(current_row)\n",
    "                current_row = []\n",
    "\n",
    "    # Convert data to a DataFrame using row_data\n",
    "    page_df = pd.DataFrame(table_data).drop(0, axis=1)\n",
    "    \n",
    "    return page_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_fighter_details(url_base, url_page):\n",
    "    # List of letters to be used to sort web page by last name\n",
    "    letters = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "\n",
    "    # Master DataFrame to store all pages fighter data\n",
    "    master_df = pd.DataFrame()\n",
    "\n",
    "    # Loop to go through all web pages based on the letter of the last name\n",
    "    for letter in letters:\n",
    "        # Set url to be scraped\n",
    "        page_url = url_base + letter + url_page\n",
    "        \n",
    "        # Send get request to url to get html text and find the table on the webpage.\n",
    "        table_body, _ = get_table_body(page_url)\n",
    "        \n",
    "        # Get fighter details from a single web page\n",
    "        page_df = get_fighter_details(table_body)\n",
    "        \n",
    "        # Add page_data to master_df\n",
    "        master_df = pd.concat([master_df, page_df], ignore_index=True, axis=0)\n",
    "        \n",
    "    return master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all functions to capture fighter details\n",
    "fighter_details = get_all_fighter_details(url_base, url_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column headers to fighter details dataframe\n",
    "cols = get_fighter_column_headers(url)\n",
    "cols = cols[:-1]\n",
    "fighter_details.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3953, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Ht.</th>\n",
       "      <th>Wt.</th>\n",
       "      <th>Reach</th>\n",
       "      <th>Stance</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>Dave</td>\n",
       "      <td>Zitanick</td>\n",
       "      <td></td>\n",
       "      <td>--</td>\n",
       "      <td>170 lbs.</td>\n",
       "      <td>--</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3949</th>\n",
       "      <td>Alex</td>\n",
       "      <td>Zuniga</td>\n",
       "      <td></td>\n",
       "      <td>--</td>\n",
       "      <td>145 lbs.</td>\n",
       "      <td>--</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3950</th>\n",
       "      <td>George</td>\n",
       "      <td>Zuniga</td>\n",
       "      <td></td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>185 lbs.</td>\n",
       "      <td>--</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3951</th>\n",
       "      <td>Allan</td>\n",
       "      <td>Zuniga</td>\n",
       "      <td>Tigre</td>\n",
       "      <td>5' 7\"</td>\n",
       "      <td>155 lbs.</td>\n",
       "      <td>70.0\"</td>\n",
       "      <td>Orthodox</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3952</th>\n",
       "      <td>Virgil</td>\n",
       "      <td>Zwicker</td>\n",
       "      <td>RezDog</td>\n",
       "      <td>6' 2\"</td>\n",
       "      <td>205 lbs.</td>\n",
       "      <td>74.0\"</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       First      Last Nickname    Ht.       Wt.  Reach    Stance   W  L  D\n",
       "3948    Dave  Zitanick              --  170 lbs.     --             5  7  0\n",
       "3949    Alex    Zuniga              --  145 lbs.     --             6  3  0\n",
       "3950  George    Zuniga           5' 9\"  185 lbs.     --             3  1  0\n",
       "3951   Allan    Zuniga    Tigre  5' 7\"  155 lbs.  70.0\"  Orthodox  13  1  0\n",
       "3952  Virgil   Zwicker   RezDog  6' 2\"  205 lbs.  74.0\"            15  6  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the shape and first 5 rows of compiled fighter dataframe\n",
    "print(fighter_details.shape)\n",
    "fighter_details.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fighter details dataframe to a pickle file            \n",
    "fighter_details.to_pickle('/Users/nathananderson/Documents/Data_Science/WebScraping/git/Web_Scraping/Pickle Files/fighter_details.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Fighter URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
